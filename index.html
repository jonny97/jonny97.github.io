<!doctype html>
<html lang="en" style="width: 100%; max-width: 960px; margin-right: auto; margin-left: auto">
  <head>
	<title>Jiannan Jiang </title>
  </head>

<body>
    <center>    
    <table cellspacing="10" border="0" cellpadding="1">
	<tr>
		<td nowrap valign="bottom">
		<h2>Jiannan Jiang
		</h2>

		Wean Hall 6213<br>
		<a href="https://www.cmu.edu/math/">
		Mathematics Department</a><br>    
		<a href="https://www.cmu.edu/mcs/">
		Mellon College of Science</a><br>
		<a href="https://www.cmu.edu/">Carnegie Mellon University</a><br>
		5000 Forbes Ave, <br>
		Pittsburgh, PA 15213
		<br><br>
		Email: jiannanj at andrew.cmu.edu
		<br>

		</td>
		<td>
			&emsp;&emsp;&emsp;
		</td>

		<td>
		<img align=top width="180" src="images/me.jpeg" alt="Jiannan Jiang">

		</td>

		<td>
			&emsp;&emsp;&emsp;
		</td>

		<td>
		<a href="#bio"><strong>Bio</strong></a><p>
		<a href="#research"><strong>Research interests</strong></a><p>
		<a href="#teaching"><strong>Teaching</strong></a>
		<p>
		<a href="resume.pdf"><strong>CV</strong></a> <p>
		<a href="https://github.com/jonny97"><strong>Github</strong></a>
	</tr>
	</table>
    </center>

<hr>
<h2><a name ="bio">Bio</a></h2>
I am a third-year PhD student at the analysis group in the Mathematics Department of Carnegie Mellon University, advised by <a href="https://www.math.cmu.edu/~hschaeff/">Hayden Schaeffer</a>.
I received my Bachelor's degree in Applied Mathematics and Computer Science at University of California, Berkeley in 2019.

<hr>
<h2><a name ="research">Research interests</a></h2>
My current research interest focuses on imposing mathematically-provable structured regularizations on data-driven models over dynamical systems. Specifically, modern machine learning techniques over dynamical systems often first impose a large hypothesis class and then regularize based on specific needs (SGD/dropout achieves statistically L2 normalization with low computational costs; L1/L0 nomalization enforces sparsity). Recent advances in deep learning focuses on imposing a pre-determined structure on neural networks based on the specific tasks, as a way to limit the hypothesis class. These regularization metohds are powerful but lack a well-structured theory to quantify the behaviors in convergence. On the other hand, the number of parameters used in these models scales exponentially as the processing and computing power grows, suggesting a potential improvement by truncating inactive computing units. I'm interested in how to scale down the number of parameters in large models while preserving its accuracy via mathematical tools. Particularly, I focus on linear/nonlinear models of dynamical systems.
<br>

	<h4>Research Directions</h4> 
	<ul>
	<li>Sparse Identification of nonlinear dynamics</li>
	<li>Reduced order modelling over nonlinear dynamics</li>
	<li>Numerical analysis</li>
	</ul>

<hr>
<h2><a name ="teaching">Teaching Experience</a></h2>
<h3>Carnegie Mellon University</h3>
	<ul>
		<li> <h4>Fall 2021</h4> MSC 21254 - Linear Algebra and Vector Calculus for Engineers, TA</li>
		<li> <h4>Spring 2021</h4>MSC 21261 - Introduction to Ordinary Differential Equations, TA</li>
		<li> <h4>Fall 2020</h4> MSC 21254 - Linear Algebra and Vector Calculus for Engineers, TA</li>
		<li> <h4>Fall 2019</h4> MSC 21122 - Integration and Approximation, TA</li>
	</ul>



<hr>
<h2><a name="links">Relavant Links</a></h2>
<ul>
<li> I have put some Matlab codes for Visualizations of ODEs in 21-261 <a href="https://github.com/jonny97/21261-selected-matlab-visualization">here</a>.
<li> A python implementation of all numerical linear algebra methods mentioned in 21-671. <a href="https://github.com/jonny97/21261-selected-matlab-visualization"> link </a>

</ul>

<hr>
This page was last updated on Sept. 2 2021<br><br>



</body>
</html>
